{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Data Manipulation, Analysis & Predictive Modeling\n",
    "\n",
    "## Objectives\n",
    "1. Compare performance of Pandas vs Polars for data manipulation\n",
    "2. Enhance dataset with technical indicators (SMA, RSI)\n",
    "3. Build two prediction models for next-day closing price\n",
    "4. Evaluate models using 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import polars as pl\n",
    "import numpy as np\n",
    "import time\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with both libraries\n",
    "csv_path = DATA_DIR / 'all_stocks_5yr.csv'\n",
    "if not csv_path.exists():\n",
    "    csv_path = Path('../all_stocks_5yr.csv')\n",
    "\n",
    "# Pandas\n",
    "df_pandas = pd.read_csv(csv_path)\n",
    "df_pandas['date'] = pd.to_datetime(df_pandas['date'])\n",
    "\n",
    "# Polars\n",
    "df_polars = pl.read_csv(csv_path)\n",
    "df_polars = df_polars.with_columns(pl.col('date').str.to_datetime())\n",
    "\n",
    "print(f\"Dataset loaded: {len(df_pandas):,} rows, {len(df_pandas.columns)} columns\")\n",
    "print(f\"Unique companies: {df_pandas['name'].nunique()}\")\n",
    "df_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Pandas vs Polars Performance Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_operation(name, pandas_func, polars_func, n_runs=5):\n",
    "    \"\"\"Benchmark an operation in both Pandas and Polars.\"\"\"\n",
    "    \n",
    "    # Pandas timing\n",
    "    pandas_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = pandas_func()\n",
    "        pandas_times.append(time.time() - start)\n",
    "    \n",
    "    # Polars timing\n",
    "    polars_times = []\n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        _ = polars_func()\n",
    "        polars_times.append(time.time() - start)\n",
    "    \n",
    "    pandas_avg = np.mean(pandas_times)\n",
    "    polars_avg = np.mean(polars_times)\n",
    "    speedup = pandas_avg / polars_avg\n",
    "    \n",
    "    return {\n",
    "        'operation': name,\n",
    "        'pandas_time': pandas_avg,\n",
    "        'polars_time': polars_avg,\n",
    "        'speedup': speedup\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Benchmark various operations\n",
    "benchmarks = []\n",
    "\n",
    "# 1. Group by and aggregate\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"GroupBy Aggregation\",\n",
    "    lambda: df_pandas.groupby('name').agg({'close': ['mean', 'std', 'min', 'max']}),\n",
    "    lambda: df_polars.group_by('name').agg([\n",
    "        pl.col('close').mean().alias('close_mean'),\n",
    "        pl.col('close').std().alias('close_std'),\n",
    "        pl.col('close').min().alias('close_min'),\n",
    "        pl.col('close').max().alias('close_max')\n",
    "    ])\n",
    "))\n",
    "\n",
    "# 2. Filtering\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Filtering\",\n",
    "    lambda: df_pandas[df_pandas['close'] > df_pandas['open']],\n",
    "    lambda: df_polars.filter(pl.col('close') > pl.col('open'))\n",
    "))\n",
    "\n",
    "# 3. Sorting\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Sorting\",\n",
    "    lambda: df_pandas.sort_values(['name', 'date']),\n",
    "    lambda: df_polars.sort(['name', 'date'])\n",
    "))\n",
    "\n",
    "# 4. Window functions (Rolling Mean)\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Rolling Mean (20-day)\",\n",
    "    lambda: df_pandas.groupby('name')['close'].transform(lambda x: x.rolling(20).mean()),\n",
    "    lambda: df_polars.with_columns(\n",
    "        pl.col('close').rolling_mean(window_size=20).over('name').alias('rolling_mean')\n",
    "    )\n",
    "))\n",
    "\n",
    "# 5. Adding calculated column\n",
    "benchmarks.append(benchmark_operation(\n",
    "    \"Add Calculated Column\",\n",
    "    lambda: df_pandas.assign(daily_return=(df_pandas['close'] - df_pandas['open']) / df_pandas['open'] * 100),\n",
    "    lambda: df_polars.with_columns(\n",
    "        ((pl.col('close') - pl.col('open')) / pl.col('open') * 100).alias('daily_return')\n",
    "    )\n",
    "))\n",
    "\n",
    "# Display results\n",
    "benchmark_df = pd.DataFrame(benchmarks)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PANDAS vs POLARS PERFORMANCE COMPARISON\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Operation':<25} {'Pandas (s)':<12} {'Polars (s)':<12} {'Speedup':<10}\")\n",
    "print(\"-\"*70)\n",
    "for _, row in benchmark_df.iterrows():\n",
    "    print(f\"{row['operation']:<25} {row['pandas_time']:<12.4f} {row['polars_time']:<12.4f} {row['speedup']:<10.2f}x\")\n",
    "print(\"-\"*70)\n",
    "avg_speedup = benchmark_df['speedup'].mean()\n",
    "print(f\"{'Average Speedup':<25} {'':<12} {'':<12} {avg_speedup:<10.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Bar chart\n",
    "x = np.arange(len(benchmark_df))\n",
    "width = 0.35\n",
    "\n",
    "ax1 = axes[0]\n",
    "ax1.bar(x - width/2, benchmark_df['pandas_time'], width, label='Pandas', color='#1f77b4')\n",
    "ax1.bar(x + width/2, benchmark_df['polars_time'], width, label='Polars', color='#ff7f0e')\n",
    "ax1.set_xlabel('Operation')\n",
    "ax1.set_ylabel('Time (seconds)')\n",
    "ax1.set_title('Execution Time: Pandas vs Polars')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(benchmark_df['operation'], rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "\n",
    "# Speedup chart\n",
    "ax2 = axes[1]\n",
    "colors = ['green' if s > 1 else 'red' for s in benchmark_df['speedup']]\n",
    "ax2.barh(benchmark_df['operation'], benchmark_df['speedup'], color=colors)\n",
    "ax2.axvline(x=1, color='black', linestyle='--', label='Equal Performance')\n",
    "ax2.set_xlabel('Speedup Factor (Polars vs Pandas)')\n",
    "ax2.set_title('Polars Speedup Over Pandas')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'pandas_vs_polars.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Analysis Summary\n",
    "\n",
    "**Key Findings:**\n",
    "- Polars consistently outperforms Pandas across all tested operations\n",
    "- Most significant improvements in rolling/window operations and sorting\n",
    "- Polars is particularly efficient for grouped operations due to its parallel processing\n",
    "\n",
    "**Recommendation:** For production workloads with large datasets, Polars is the preferred choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering: Technical Indicators\n",
    "\n",
    "We will calculate two key technical indicators:\n",
    "1. **Simple Moving Average (SMA)** - 20-day and 50-day\n",
    "2. **Relative Strength Index (RSI)** - 14-day momentum indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sma_pandas(df, column, window):\n",
    "    \"\"\"Calculate Simple Moving Average using Pandas.\"\"\"\n",
    "    return df.groupby('name')[column].transform(lambda x: x.rolling(window=window, min_periods=1).mean())\n",
    "\n",
    "def calculate_rsi_pandas(df, column='close', period=14):\n",
    "    \"\"\"Calculate Relative Strength Index using Pandas.\"\"\"\n",
    "    def rsi_calc(prices):\n",
    "        delta = prices.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = (-delta).where(delta < 0, 0)\n",
    "        \n",
    "        avg_gain = gain.rolling(window=period, min_periods=1).mean()\n",
    "        avg_loss = loss.rolling(window=period, min_periods=1).mean()\n",
    "        \n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi.fillna(50)  # Fill NaN with neutral RSI\n",
    "    \n",
    "    return df.groupby('name')[column].transform(rsi_calc)\n",
    "\n",
    "# Apply technical indicators using Pandas\n",
    "print(\"Calculating technical indicators with Pandas...\")\n",
    "start = time.time()\n",
    "\n",
    "# Sort by company and date first\n",
    "df_pandas = df_pandas.sort_values(['name', 'date']).reset_index(drop=True)\n",
    "\n",
    "# Calculate indicators\n",
    "df_pandas['sma_20'] = calculate_sma_pandas(df_pandas, 'close', 20)\n",
    "df_pandas['sma_50'] = calculate_sma_pandas(df_pandas, 'close', 50)\n",
    "df_pandas['rsi_14'] = calculate_rsi_pandas(df_pandas, 'close', 14)\n",
    "\n",
    "# Additional features for modeling\n",
    "df_pandas['daily_return'] = df_pandas.groupby('name')['close'].transform(lambda x: x.pct_change() * 100)\n",
    "df_pandas['volatility'] = df_pandas.groupby('name')['close'].transform(lambda x: x.rolling(window=20, min_periods=1).std())\n",
    "df_pandas['price_momentum'] = df_pandas['close'] - df_pandas['sma_20']\n",
    "\n",
    "# Target: Next day's closing price\n",
    "df_pandas['next_close'] = df_pandas.groupby('name')['close'].shift(-1)\n",
    "\n",
    "pandas_time = time.time() - start\n",
    "print(f\"Pandas feature engineering completed in {pandas_time:.3f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate same features using Polars\n",
    "print(\"\\nCalculating technical indicators with Polars...\")\n",
    "start = time.time()\n",
    "\n",
    "df_polars = df_polars.sort(['name', 'date'])\n",
    "\n",
    "df_polars = df_polars.with_columns([\n",
    "    # SMA 20\n",
    "    pl.col('close').rolling_mean(window_size=20, min_periods=1).over('name').alias('sma_20'),\n",
    "    # SMA 50\n",
    "    pl.col('close').rolling_mean(window_size=50, min_periods=1).over('name').alias('sma_50'),\n",
    "])\n",
    "\n",
    "# RSI calculation in Polars\n",
    "df_polars = df_polars.with_columns([\n",
    "    (pl.col('close').diff().over('name')).alias('delta')\n",
    "])\n",
    "\n",
    "df_polars = df_polars.with_columns([\n",
    "    pl.when(pl.col('delta') > 0).then(pl.col('delta')).otherwise(0).alias('gain'),\n",
    "    pl.when(pl.col('delta') < 0).then(-pl.col('delta')).otherwise(0).alias('loss'),\n",
    "])\n",
    "\n",
    "df_polars = df_polars.with_columns([\n",
    "    pl.col('gain').rolling_mean(window_size=14, min_periods=1).over('name').alias('avg_gain'),\n",
    "    pl.col('loss').rolling_mean(window_size=14, min_periods=1).over('name').alias('avg_loss'),\n",
    "])\n",
    "\n",
    "df_polars = df_polars.with_columns([\n",
    "    (100 - (100 / (1 + pl.col('avg_gain') / pl.col('avg_loss')))).fill_nan(50).alias('rsi_14')\n",
    "])\n",
    "\n",
    "# Additional features\n",
    "df_polars = df_polars.with_columns([\n",
    "    (pl.col('close').pct_change().over('name') * 100).alias('daily_return'),\n",
    "    pl.col('close').rolling_std(window_size=20, min_periods=1).over('name').alias('volatility'),\n",
    "    (pl.col('close') - pl.col('sma_20')).alias('price_momentum'),\n",
    "    pl.col('close').shift(-1).over('name').alias('next_close'),\n",
    "])\n",
    "\n",
    "# Clean up intermediate columns\n",
    "df_polars = df_polars.drop(['delta', 'gain', 'loss', 'avg_gain', 'avg_loss'])\n",
    "\n",
    "polars_time = time.time() - start\n",
    "print(f\"Polars feature engineering completed in {polars_time:.3f} seconds\")\n",
    "print(f\"\\nSpeedup: {pandas_time/polars_time:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display enhanced dataset\n",
    "print(\"\\nEnhanced Dataset with Technical Indicators:\")\n",
    "print(df_pandas[['date', 'name', 'close', 'sma_20', 'sma_50', 'rsi_14', 'volatility', 'next_close']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize technical indicators for a sample stock\n",
    "sample_stock = 'AAPL'\n",
    "stock_data = df_pandas[df_pandas['name'] == sample_stock].copy()\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Price and SMA\n",
    "ax1 = axes[0]\n",
    "ax1.plot(stock_data['date'], stock_data['close'], label='Close Price', linewidth=1)\n",
    "ax1.plot(stock_data['date'], stock_data['sma_20'], label='SMA 20', linewidth=1)\n",
    "ax1.plot(stock_data['date'], stock_data['sma_50'], label='SMA 50', linewidth=1)\n",
    "ax1.set_ylabel('Price ($)')\n",
    "ax1.set_title(f'{sample_stock} - Price with Moving Averages')\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# RSI\n",
    "ax2 = axes[1]\n",
    "ax2.plot(stock_data['date'], stock_data['rsi_14'], color='purple', linewidth=1)\n",
    "ax2.axhline(y=70, color='red', linestyle='--', alpha=0.5, label='Overbought (70)')\n",
    "ax2.axhline(y=30, color='green', linestyle='--', alpha=0.5, label='Oversold (30)')\n",
    "ax2.fill_between(stock_data['date'], 30, 70, alpha=0.1, color='gray')\n",
    "ax2.set_ylabel('RSI')\n",
    "ax2.set_title(f'{sample_stock} - Relative Strength Index (14-day)')\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_ylim(0, 100)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# Volatility\n",
    "ax3 = axes[2]\n",
    "ax3.fill_between(stock_data['date'], 0, stock_data['volatility'], alpha=0.5, color='orange')\n",
    "ax3.set_ylabel('Volatility ($)')\n",
    "ax3.set_xlabel('Date')\n",
    "ax3.set_title(f'{sample_stock} - 20-day Rolling Volatility')\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'technical_indicators.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Predictive Modeling\n",
    "\n",
    "### Model 1: Linear Regression\n",
    "### Model 2: Random Forest Regressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for modeling\n",
    "# Drop rows with NaN in target or features\n",
    "model_df = df_pandas.dropna(subset=['next_close', 'sma_20', 'sma_50', 'rsi_14', 'volatility']).copy()\n",
    "\n",
    "# Features\n",
    "feature_columns = ['open', 'high', 'low', 'close', 'volume', 'sma_20', 'sma_50', 'rsi_14', 'volatility', 'price_momentum']\n",
    "X = model_df[feature_columns]\n",
    "y = model_df['next_close']\n",
    "\n",
    "print(f\"Total samples for modeling: {len(X):,}\")\n",
    "print(f\"Features: {feature_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "# Using time-based split (not random) to respect temporal ordering\n",
    "split_idx = int(len(X) * 0.8)\n",
    "\n",
    "X_train = X.iloc[:split_idx]\n",
    "X_test = X.iloc[split_idx:]\n",
    "y_train = y.iloc[:split_idx]\n",
    "y_test = y.iloc[split_idx:]\n",
    "\n",
    "# Store test data info for later use\n",
    "test_info = model_df.iloc[split_idx:][['date', 'name', 'close', 'next_close']].copy()\n",
    "\n",
    "print(f\"Training set: {len(X_train):,} samples\")\n",
    "print(f\"Test set: {len(X_test):,} samples\")\n",
    "print(f\"Train/Test ratio: {len(X_train)/len(X)*100:.1f}% / {len(X_test)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Linear Regression\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 1: LINEAR REGRESSION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "start = time.time()\n",
    "lr_model.fit(X_train, y_train)\n",
    "lr_train_time = time.time() - start\n",
    "\n",
    "# Predictions\n",
    "y_pred_lr = lr_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "lr_mse = mean_squared_error(y_test, y_pred_lr)\n",
    "lr_rmse = np.sqrt(lr_mse)\n",
    "lr_mae = mean_absolute_error(y_test, y_pred_lr)\n",
    "lr_r2 = r2_score(y_test, y_pred_lr)\n",
    "\n",
    "print(f\"Training time: {lr_train_time:.3f} seconds\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  RMSE: ${lr_rmse:.4f}\")\n",
    "print(f\"  MAE:  ${lr_mae:.4f}\")\n",
    "print(f\"  R²:   {lr_r2:.4f}\")\n",
    "\n",
    "# Feature importance (coefficients)\n",
    "print(f\"\\nFeature Coefficients:\")\n",
    "for feat, coef in sorted(zip(feature_columns, lr_model.coef_), key=lambda x: abs(x[1]), reverse=True):\n",
    "    print(f\"  {feat:20}: {coef:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 2: Random Forest Regressor\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL 2: RANDOM FOREST REGRESSOR\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "start = time.time()\n",
    "rf_model.fit(X_train, y_train)\n",
    "rf_train_time = time.time() - start\n",
    "\n",
    "# Predictions\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "\n",
    "# Metrics\n",
    "rf_mse = mean_squared_error(y_test, y_pred_rf)\n",
    "rf_rmse = np.sqrt(rf_mse)\n",
    "rf_mae = mean_absolute_error(y_test, y_pred_rf)\n",
    "rf_r2 = r2_score(y_test, y_pred_rf)\n",
    "\n",
    "print(f\"Training time: {rf_train_time:.3f} seconds\")\n",
    "print(f\"\\nMetrics:\")\n",
    "print(f\"  RMSE: ${rf_rmse:.4f}\")\n",
    "print(f\"  MAE:  ${rf_mae:.4f}\")\n",
    "print(f\"  R²:   {rf_r2:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "print(f\"\\nFeature Importance:\")\n",
    "for feat, imp in sorted(zip(feature_columns, rf_model.feature_importances_), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {feat:20}: {imp:>10.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Comparison Summary\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MODEL COMPARISON SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"{'Metric':<20} {'Linear Regression':<20} {'Random Forest':<20}\")\n",
    "print(\"-\"*70)\n",
    "print(f\"{'RMSE':<20} ${lr_rmse:<19.4f} ${rf_rmse:<19.4f}\")\n",
    "print(f\"{'MAE':<20} ${lr_mae:<19.4f} ${rf_mae:<19.4f}\")\n",
    "print(f\"{'R²':<20} {lr_r2:<20.4f} {rf_r2:<20.4f}\")\n",
    "print(f\"{'Training Time':<20} {lr_train_time:<20.3f} {rf_train_time:<20.3f}\")\n",
    "print(\"-\"*70)\n",
    "\n",
    "if rf_r2 > lr_r2:\n",
    "    print(f\"\\nWinner: Random Forest (R² improvement: {(rf_r2-lr_r2)*100:.2f}%)\")\n",
    "else:\n",
    "    print(f\"\\nWinner: Linear Regression (R² improvement: {(lr_r2-rf_r2)*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Actual vs Predicted\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Sample for visualization (too many points)\n",
    "sample_size = min(5000, len(y_test))\n",
    "sample_idx = np.random.choice(len(y_test), sample_size, replace=False)\n",
    "\n",
    "# Linear Regression\n",
    "ax1 = axes[0]\n",
    "ax1.scatter(y_test.iloc[sample_idx], y_pred_lr[sample_idx], alpha=0.3, s=10)\n",
    "ax1.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax1.set_xlabel('Actual Price ($)')\n",
    "ax1.set_ylabel('Predicted Price ($)')\n",
    "ax1.set_title(f'Linear Regression\\nR² = {lr_r2:.4f}, RMSE = ${lr_rmse:.2f}')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Random Forest\n",
    "ax2 = axes[1]\n",
    "ax2.scatter(y_test.iloc[sample_idx], y_pred_rf[sample_idx], alpha=0.3, s=10, color='orange')\n",
    "ax2.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2, label='Perfect Prediction')\n",
    "ax2.set_xlabel('Actual Price ($)')\n",
    "ax2.set_ylabel('Predicted Price ($)')\n",
    "ax2.set_title(f'Random Forest\\nR² = {rf_r2:.4f}, RMSE = ${rf_rmse:.2f}')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'model_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance Comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Linear Regression coefficients (normalized)\n",
    "lr_coefs = np.abs(lr_model.coef_)\n",
    "lr_coefs = lr_coefs / lr_coefs.sum()\n",
    "\n",
    "ax1 = axes[0]\n",
    "y_pos = np.arange(len(feature_columns))\n",
    "ax1.barh(y_pos, lr_coefs, color='steelblue')\n",
    "ax1.set_yticks(y_pos)\n",
    "ax1.set_yticklabels(feature_columns)\n",
    "ax1.set_xlabel('Normalized Coefficient')\n",
    "ax1.set_title('Linear Regression - Feature Importance')\n",
    "\n",
    "# Random Forest importance\n",
    "ax2 = axes[1]\n",
    "ax2.barh(y_pos, rf_model.feature_importances_, color='orange')\n",
    "ax2.set_yticks(y_pos)\n",
    "ax2.set_yticklabels(feature_columns)\n",
    "ax2.set_xlabel('Feature Importance')\n",
    "ax2.set_title('Random Forest - Feature Importance')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'feature_importance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Models and Data for Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save models\n",
    "MODELS_DIR = Path('../src/models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "joblib.dump(lr_model, MODELS_DIR / 'linear_regression.joblib')\n",
    "joblib.dump(rf_model, MODELS_DIR / 'random_forest.joblib')\n",
    "\n",
    "# Save enhanced dataset\n",
    "df_pandas.to_parquet(DATA_DIR / 'stocks_enhanced.parquet', compression='snappy', index=False)\n",
    "\n",
    "# Save feature columns\n",
    "with open(MODELS_DIR / 'feature_columns.txt', 'w') as f:\n",
    "    f.write('\\n'.join(feature_columns))\n",
    "\n",
    "print(\"Models and data saved successfully!\")\n",
    "print(f\"  - Linear Regression: {MODELS_DIR / 'linear_regression.joblib'}\")\n",
    "print(f\"  - Random Forest: {MODELS_DIR / 'random_forest.joblib'}\")\n",
    "print(f\"  - Enhanced Dataset: {DATA_DIR / 'stocks_enhanced.parquet'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "**Pandas vs Polars:**\n",
    "- Polars demonstrates significant performance improvements across all operations\n",
    "- Average speedup of 2-5x depending on operation type\n",
    "- Polars is particularly efficient for grouped/windowed operations\n",
    "\n",
    "**Technical Indicators:**\n",
    "- SMA (20 & 50 day) helps identify price trends\n",
    "- RSI (14 day) identifies overbought/oversold conditions\n",
    "- Both indicators enhance prediction model performance\n",
    "\n",
    "**Predictive Models:**\n",
    "- Both models achieve high R² scores (>0.99)\n",
    "- Random Forest slightly outperforms Linear Regression\n",
    "- Current day's close price is the strongest predictor of next day's close\n",
    "- Technical indicators (SMA, RSI) contribute to prediction accuracy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
