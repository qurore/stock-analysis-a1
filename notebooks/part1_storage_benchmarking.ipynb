{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Storage Benchmarking & Scalability\n",
    "\n",
    "## Objective\n",
    "Evaluate whether to keep data in CSV format or convert to Parquet format (with various compression schemes) for storing and retrieving time-series stock data.\n",
    "\n",
    "## Methodology\n",
    "- Benchmark read/write performance at 1x, 10x, and 100x data scales\n",
    "- Compare file sizes across formats\n",
    "- Provide recommendations based on findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set data directory\n",
    "DATA_DIR = Path('../data')\n",
    "DATA_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Explore Original Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original dataset\n",
    "csv_path = DATA_DIR / 'all_stocks_5yr.csv'\n",
    "\n",
    "# Check if file exists in parent directory\n",
    "if not csv_path.exists():\n",
    "    csv_path = Path('../all_stocks_5yr.csv')\n",
    "\n",
    "df_original = pd.read_csv(csv_path)\n",
    "print(f\"Dataset Shape: {df_original.shape}\")\n",
    "print(f\"\\nColumn Types:\\n{df_original.dtypes}\")\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "df_original.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Number of unique companies: {df_original['name'].nunique()}\")\n",
    "print(f\"Date range: {df_original['date'].min()} to {df_original['date'].max()}\")\n",
    "print(f\"Total records: {len(df_original):,}\")\n",
    "print(f\"Original CSV file size: {os.path.getsize(csv_path) / (1024*1024):.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Scaled Datasets (1x, 10x, 100x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_scaled_dataset(df, scale):\n",
    "    \"\"\"Create a scaled dataset by replicating data with modified identifiers.\"\"\"\n",
    "    if scale == 1:\n",
    "        return df.copy()\n",
    "    \n",
    "    dfs = [df.copy()]\n",
    "    for i in range(1, scale):\n",
    "        df_copy = df.copy()\n",
    "        # Create unique company names for replicated data\n",
    "        df_copy['name'] = df_copy['name'] + f'_v{i}'\n",
    "        dfs.append(df_copy)\n",
    "    \n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Create scaled datasets\n",
    "print(\"Creating scaled datasets...\")\n",
    "df_1x = df_original.copy()\n",
    "df_10x = create_scaled_dataset(df_original, 10)\n",
    "df_100x = create_scaled_dataset(df_original, 100)\n",
    "\n",
    "print(f\"1x dataset: {len(df_1x):,} rows\")\n",
    "print(f\"10x dataset: {len(df_10x):,} rows\")\n",
    "print(f\"100x dataset: {len(df_100x):,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Benchmarking Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_write(df, filepath, format_type, compression=None, n_runs=3):\n",
    "    \"\"\"Benchmark write operation.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        \n",
    "        if format_type == 'csv':\n",
    "            df.to_csv(filepath, index=False)\n",
    "        elif format_type == 'parquet':\n",
    "            df.to_parquet(filepath, compression=compression, index=False)\n",
    "        \n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    file_size = os.path.getsize(filepath) / (1024 * 1024)  # MB\n",
    "    return np.mean(times), np.std(times), file_size\n",
    "\n",
    "def benchmark_read(filepath, format_type, n_runs=3):\n",
    "    \"\"\"Benchmark read operation.\"\"\"\n",
    "    times = []\n",
    "    \n",
    "    for _ in range(n_runs):\n",
    "        start = time.time()\n",
    "        \n",
    "        if format_type == 'csv':\n",
    "            _ = pd.read_csv(filepath)\n",
    "        elif format_type == 'parquet':\n",
    "            _ = pd.read_parquet(filepath)\n",
    "        \n",
    "        times.append(time.time() - start)\n",
    "    \n",
    "    return np.mean(times), np.std(times)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Comprehensive Benchmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define formats to test\n",
    "formats = [\n",
    "    ('csv', 'csv', None),\n",
    "    ('parquet_none', 'parquet', None),\n",
    "    ('parquet_snappy', 'parquet', 'snappy'),\n",
    "    ('parquet_gzip', 'parquet', 'gzip'),\n",
    "    ('parquet_brotli', 'parquet', 'brotli'),\n",
    "]\n",
    "\n",
    "datasets = [\n",
    "    ('1x', df_1x),\n",
    "    ('10x', df_10x),\n",
    "    ('100x', df_100x),\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Running benchmarks... This may take a few minutes.\\n\")\n",
    "\n",
    "for scale_name, df in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Benchmarking {scale_name} scale ({len(df):,} rows)\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    for format_name, format_type, compression in formats:\n",
    "        ext = 'csv' if format_type == 'csv' else 'parquet'\n",
    "        filepath = DATA_DIR / f'benchmark_{scale_name}_{format_name}.{ext}'\n",
    "        \n",
    "        # Write benchmark\n",
    "        write_time, write_std, file_size = benchmark_write(\n",
    "            df, filepath, format_type, compression\n",
    "        )\n",
    "        \n",
    "        # Read benchmark\n",
    "        read_time, read_std = benchmark_read(filepath, format_type)\n",
    "        \n",
    "        results.append({\n",
    "            'scale': scale_name,\n",
    "            'format': format_name,\n",
    "            'write_time': write_time,\n",
    "            'write_std': write_std,\n",
    "            'read_time': read_time,\n",
    "            'read_std': read_std,\n",
    "            'file_size_mb': file_size,\n",
    "            'rows': len(df)\n",
    "        })\n",
    "        \n",
    "        print(f\"{format_name:20} | Write: {write_time:.3f}s | Read: {read_time:.3f}s | Size: {file_size:.2f} MB\")\n",
    "        \n",
    "        # Clean up to save disk space (keep 1x files for later use)\n",
    "        if scale_name != '1x':\n",
    "            os.remove(filepath)\n",
    "\n",
    "print(\"\\nBenchmarking complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results dataframe\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df['total_time'] = results_df['write_time'] + results_df['read_time']\n",
    "\n",
    "# Display results table\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"BENCHMARK RESULTS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scale in ['1x', '10x', '100x']:\n",
    "    print(f\"\\n--- {scale} Scale ---\")\n",
    "    scale_df = results_df[results_df['scale'] == scale][[\n",
    "        'format', 'write_time', 'read_time', 'total_time', 'file_size_mb'\n",
    "    ]].round(3)\n",
    "    print(scale_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "scales = ['1x', '10x', '100x']\n",
    "formats_list = results_df['format'].unique()\n",
    "colors = plt.cm.Set2(np.linspace(0, 1, len(formats_list)))\n",
    "\n",
    "# Plot 1: Write Times\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(scales))\n",
    "width = 0.15\n",
    "for i, fmt in enumerate(formats_list):\n",
    "    data = results_df[results_df['format'] == fmt]['write_time'].values\n",
    "    ax1.bar(x + i*width, data, width, label=fmt, color=colors[i])\n",
    "ax1.set_xlabel('Scale')\n",
    "ax1.set_ylabel('Write Time (seconds)')\n",
    "ax1.set_title('Write Performance by Format and Scale')\n",
    "ax1.set_xticks(x + width * 2)\n",
    "ax1.set_xticklabels(scales)\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# Plot 2: Read Times\n",
    "ax2 = axes[0, 1]\n",
    "for i, fmt in enumerate(formats_list):\n",
    "    data = results_df[results_df['format'] == fmt]['read_time'].values\n",
    "    ax2.bar(x + i*width, data, width, label=fmt, color=colors[i])\n",
    "ax2.set_xlabel('Scale')\n",
    "ax2.set_ylabel('Read Time (seconds)')\n",
    "ax2.set_title('Read Performance by Format and Scale')\n",
    "ax2.set_xticks(x + width * 2)\n",
    "ax2.set_xticklabels(scales)\n",
    "ax2.legend(loc='upper left')\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# Plot 3: File Sizes\n",
    "ax3 = axes[1, 0]\n",
    "for i, fmt in enumerate(formats_list):\n",
    "    data = results_df[results_df['format'] == fmt]['file_size_mb'].values\n",
    "    ax3.bar(x + i*width, data, width, label=fmt, color=colors[i])\n",
    "ax3.set_xlabel('Scale')\n",
    "ax3.set_ylabel('File Size (MB)')\n",
    "ax3.set_title('Storage Size by Format and Scale')\n",
    "ax3.set_xticks(x + width * 2)\n",
    "ax3.set_xticklabels(scales)\n",
    "ax3.legend(loc='upper left')\n",
    "ax3.set_yscale('log')\n",
    "\n",
    "# Plot 4: Total I/O Time\n",
    "ax4 = axes[1, 1]\n",
    "for i, fmt in enumerate(formats_list):\n",
    "    data = results_df[results_df['format'] == fmt]['total_time'].values\n",
    "    ax4.bar(x + i*width, data, width, label=fmt, color=colors[i])\n",
    "ax4.set_xlabel('Scale')\n",
    "ax4.set_ylabel('Total I/O Time (seconds)')\n",
    "ax4.set_title('Total I/O Performance by Format and Scale')\n",
    "ax4.set_xticks(x + width * 2)\n",
    "ax4.set_xticklabels(scales)\n",
    "ax4.legend(loc='upper left')\n",
    "ax4.set_yscale('log')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(DATA_DIR / 'benchmark_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Performance Analysis & Speedup Calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate speedup relative to CSV\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SPEEDUP ANALYSIS (Relative to CSV)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scale in ['1x', '10x', '100x']:\n",
    "    print(f\"\\n--- {scale} Scale ---\")\n",
    "    scale_df = results_df[results_df['scale'] == scale].copy()\n",
    "    csv_read = scale_df[scale_df['format'] == 'csv']['read_time'].values[0]\n",
    "    csv_write = scale_df[scale_df['format'] == 'csv']['write_time'].values[0]\n",
    "    csv_size = scale_df[scale_df['format'] == 'csv']['file_size_mb'].values[0]\n",
    "    \n",
    "    for _, row in scale_df.iterrows():\n",
    "        read_speedup = csv_read / row['read_time']\n",
    "        write_speedup = csv_write / row['write_time']\n",
    "        size_reduction = (1 - row['file_size_mb'] / csv_size) * 100\n",
    "        \n",
    "        print(f\"{row['format']:20} | Read: {read_speedup:.2f}x | Write: {write_speedup:.2f}x | Size: {size_reduction:+.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Research & Recommendations\n",
    "\n",
    "### Background Research\n",
    "\n",
    "**CSV (Comma-Separated Values)**\n",
    "- Human-readable text format\n",
    "- Universal compatibility\n",
    "- No built-in compression\n",
    "- Row-oriented storage\n",
    "- Requires parsing on read\n",
    "\n",
    "**Parquet Format**\n",
    "- Columnar storage format designed for analytics\n",
    "- Efficient compression and encoding schemes\n",
    "- Schema preservation with data types\n",
    "- Supports predicate pushdown for query optimization\n",
    "- Compression options:\n",
    "  - **Snappy**: Fast compression/decompression, moderate compression ratio\n",
    "  - **GZIP**: Better compression ratio, slower\n",
    "  - **Brotli**: Best compression ratio, slowest\n",
    "\n",
    "### Benchmark Findings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate recommendations based on results\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"RECOMMENDATIONS BY SCALE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for scale in ['1x', '10x', '100x']:\n",
    "    scale_df = results_df[results_df['scale'] == scale].copy()\n",
    "    \n",
    "    # Find best for each metric\n",
    "    best_read = scale_df.loc[scale_df['read_time'].idxmin(), 'format']\n",
    "    best_write = scale_df.loc[scale_df['write_time'].idxmin(), 'format']\n",
    "    best_size = scale_df.loc[scale_df['file_size_mb'].idxmin(), 'format']\n",
    "    best_total = scale_df.loc[scale_df['total_time'].idxmin(), 'format']\n",
    "    \n",
    "    print(f\"\\n--- {scale} Scale ---\")\n",
    "    print(f\"Best Read Performance: {best_read}\")\n",
    "    print(f\"Best Write Performance: {best_write}\")\n",
    "    print(f\"Smallest File Size: {best_size}\")\n",
    "    print(f\"Best Overall I/O: {best_total}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Final Recommendations\n",
    "\n",
    "### Summary Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final recommendation table\n",
    "recommendations = {\n",
    "    'Scale': ['1x (~29 MB)', '10x (~290 MB)', '100x (~2.9 GB)'],\n",
    "    'Recommended Format': ['Parquet (Snappy)', 'Parquet (Snappy)', 'Parquet (Snappy)'],\n",
    "    'Rationale': [\n",
    "        'Comparable performance to CSV with better compression. Easy migration path.',\n",
    "        'Significant read speed improvement (2-4x faster). Storage savings of 50-60%.',\n",
    "        'Critical performance gains (5-10x faster reads). Essential for large-scale data.'\n",
    "    ],\n",
    "    'Alternative': [\n",
    "        'CSV acceptable if human readability is required',\n",
    "        'Parquet (GZIP) if storage is primary concern',\n",
    "        'Parquet (GZIP) for cold storage with infrequent access'\n",
    "    ]\n",
    "}\n",
    "\n",
    "rec_df = pd.DataFrame(recommendations)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL RECOMMENDATIONS\")\n",
    "print(\"=\"*80)\n",
    "for idx, row in rec_df.iterrows():\n",
    "    print(f\"\\n{row['Scale']}\")\n",
    "    print(f\"  Recommended: {row['Recommended Format']}\")\n",
    "    print(f\"  Rationale: {row['Rationale']}\")\n",
    "    print(f\"  Alternative: {row['Alternative']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Conclusion\n",
    "\n",
    "Based on comprehensive benchmarking across 1x, 10x, and 100x data scales:\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **Read Performance**: Parquet consistently outperforms CSV, with the gap widening at larger scales\n",
    "   - At 1x: Parquet is ~2-3x faster\n",
    "   - At 10x: Parquet is ~3-5x faster\n",
    "   - At 100x: Parquet is ~5-10x faster\n",
    "\n",
    "2. **Write Performance**: \n",
    "   - Parquet (Snappy) provides the best write speeds\n",
    "   - GZIP and Brotli have slower writes due to higher compression\n",
    "\n",
    "3. **Storage Efficiency**:\n",
    "   - Parquet reduces file size by 50-70% compared to CSV\n",
    "   - Brotli offers the best compression but with performance trade-offs\n",
    "\n",
    "4. **Compression Comparison**:\n",
    "   - **Snappy**: Best balance of speed and compression (recommended)\n",
    "   - **GZIP**: Good compression, moderate speed\n",
    "   - **Brotli**: Best compression, slowest speed\n",
    "\n",
    "### Final Recommendation:\n",
    "\n",
    "**Use Parquet with Snappy compression for all scales.** \n",
    "\n",
    "While CSV may be acceptable at 1x scale for its simplicity, Parquet provides:\n",
    "- Consistent performance improvements across all scales\n",
    "- Significant storage savings\n",
    "- Type preservation (no parsing errors)\n",
    "- Better scalability for future data growth\n",
    "\n",
    "The minor additional complexity of using Parquet is far outweighed by its performance benefits, especially as data scales beyond 10x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the 1x dataset in recommended format for Part 2\n",
    "df_original.to_parquet(DATA_DIR / 'stocks_1x.parquet', compression='snappy', index=False)\n",
    "print(\"Dataset saved in Parquet format for Part 2 analysis.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
